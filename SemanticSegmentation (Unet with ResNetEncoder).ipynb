{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "CUDA_LAUNCH_BLOCKING=\"1\"\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def recursive_glob(rootdir, suffix):\n",
    "    \n",
    "   \n",
    "    matches = []\n",
    "    for path, subdirs, files in os.walk(rootdir):\n",
    "        for name in files:\n",
    "         if suffix in name:\n",
    "          matches.append(os.path.join(path,name))\n",
    "            \n",
    "                \n",
    "\n",
    "    return matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision.transforms.functional as TF\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class CityscapesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, split='train', mode='fine', augment=False):\n",
    "\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.mode = 'gtFine' if mode == 'fine' else 'gtCoarse'\n",
    "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "        self.targets_dir = os.path.join(self.root, self.mode, split)\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "        self.mapping = {\n",
    "            0: 0,  # unlabeled\n",
    "            1: 0,  # ego vehicle\n",
    "            2: 0,  # rect border\n",
    "            3: 0,  # out of roi\n",
    "            4: 0,  # static\n",
    "            5: 0,  # dynamic\n",
    "            6: 0,  # ground\n",
    "            7: 1,  # road\n",
    "            8: 0,  # sidewalk\n",
    "            9: 0,  # parking\n",
    "            10: 0,  # rail track\n",
    "            11: 0,  # building\n",
    "            12: 0,  # wall\n",
    "            13: 0,  # fence\n",
    "            14: 0,  # guard rail\n",
    "            15: 0,  # bridge\n",
    "            16: 0,  # tunnel\n",
    "            17: 0,  # pole\n",
    "            18: 0,  # polegroup\n",
    "            19: 0,  # traffic light\n",
    "            20: 0,  # traffic sign\n",
    "            21: 0,  # vegetation\n",
    "            22: 0,  # terrain\n",
    "            23: 2,  # sky\n",
    "            24: 0,  # person\n",
    "            25: 0,  # rider\n",
    "            26: 3,  # car\n",
    "            27: 0,  # truck\n",
    "            28: 0,  # bus\n",
    "            29: 0,  # caravan\n",
    "            30: 0,  # trailer\n",
    "            31: 0,  # train\n",
    "            32: 0,  # motorcycle\n",
    "            33: 0,  # bicycle\n",
    "            -1: 0  # licenseplate\n",
    "        }\n",
    "        self.mappingrgb = {\n",
    "            0: (255, 0, 0),  # unlabeled\n",
    "            1: (255, 0, 0),  # ego vehicle\n",
    "            2: (255, 0, 0),  # rect border\n",
    "            3: (255, 0, 0),  # out of roi\n",
    "            4: (255, 0, 0),  # static\n",
    "            5: (255, 0, 0),  # dynamic\n",
    "            6: (255, 0, 0),  # ground\n",
    "            7: (0, 255, 0),  # road\n",
    "            8: (255, 0, 0),  # sidewalk\n",
    "            9: (255, 0, 0),  # parking\n",
    "            10: (255, 0, 0),  # rail track\n",
    "            11: (255, 0, 0),  # building\n",
    "            12: (255, 0, 0),  # wall\n",
    "            13: (255, 0, 0),  # fence\n",
    "            14: (255, 0, 0),  # guard rail\n",
    "            15: (255, 0, 0),  # bridge\n",
    "            16: (255, 0, 0),  # tunnel\n",
    "            17: (255, 0, 0),  # pole\n",
    "            18: (255, 0, 0),  # polegroup\n",
    "            19: (255, 0, 0),  # traffic light\n",
    "            20: (255, 0, 0),  # traffic sign\n",
    "            21: (255, 0, 0),  # vegetation\n",
    "            22: (255, 0, 0),  # terrain\n",
    "            23: (0, 0, 255),  # sky\n",
    "            24: (255, 0, 0),  # person\n",
    "            25: (255, 0, 0),  # rider\n",
    "            26: (255, 255, 0),  # car\n",
    "            27: (255, 0, 0),  # truck\n",
    "            28: (255, 0, 0),  # bus\n",
    "            29: (255, 0, 0),  # caravan\n",
    "            30: (255, 0, 0),  # trailer\n",
    "            31: (255, 0, 0),  # train\n",
    "            32: (255, 0, 0),  # motorcycle\n",
    "            33: (255, 0, 0),  # bicycle\n",
    "            -1: (255, 0, 0)  # licenseplate\n",
    "        }\n",
    "\n",
    "        # Ensure that this matches the above mapping!#!@#!@#\n",
    "        # For example 4 classes, means we should map to the ids=(0,1,2,3)\n",
    "        # This is used to specify how many outputs the network should product...\n",
    "        self.num_classes = 4\n",
    "\n",
    "        # =============================================\n",
    "        # Check that inputs are valid\n",
    "        # =============================================\n",
    "        if mode not in ['fine', 'coarse']:\n",
    "            raise ValueError('Invalid mode! Please use mode=\"fine\" or mode=\"coarse\"')\n",
    "        if mode == 'fine' and split not in ['train', 'test', 'val']:\n",
    "            raise ValueError('Invalid split for mode \"fine\"! Please use split=\"train\", split=\"test\" or split=\"val\"')\n",
    "        elif mode == 'coarse' and split not in ['train', 'train_extra', 'val']:\n",
    "            raise ValueError('Invalid split for mode \"coarse\"! Please use split=\"train\", split=\"train_extra\" or split=\"val\"')\n",
    "        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.targets_dir):\n",
    "            raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n",
    "                               ' specified \"split\" and \"mode\" are inside the \"root\" directory')\n",
    "\n",
    "        # =============================================\n",
    "        # Read in the paths to all images\n",
    "        # =============================================\n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            target_dir = os.path.join(self.targets_dir, city)\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                self.images.append(os.path.join(img_dir, file_name))\n",
    "                target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0], '{}_labelIds.png'.format(self.mode))\n",
    "                # target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0], '{}_color.png'.format(self.mode))\n",
    "                self.targets.append(os.path.join(target_dir, target_name))\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of images: {}\\n'.format(self.__len__())\n",
    "        fmt_str += '    Split: {}\\n'.format(self.split)\n",
    "        fmt_str += '    Mode: {}\\n'.format(self.mode)\n",
    "        fmt_str += '    Augment: {}\\n'.format(self.augment)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        return fmt_str\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def mask_to_class(self, mask):\n",
    "        '''\n",
    "        Given the cityscapes dataset, this maps to a 0..classes numbers.\n",
    "        This is because we are using a subset of all masks, so we have this \"mapping\" function.\n",
    "        This mapping function is used to map all the standard ids into the smaller subset.\n",
    "        '''\n",
    "        maskimg = torch.zeros((mask.size()[0], mask.size()[1]), dtype=torch.uint8)\n",
    "        for k in self.mapping:\n",
    "            maskimg[mask == k] = self.mapping[k]\n",
    "        return maskimg\n",
    "\n",
    "    def mask_to_rgb(self, mask):\n",
    "        '''\n",
    "        Given the Cityscapes mask file, this converts the ids into rgb colors.\n",
    "        This is needed as we are interested in a sub-set of labels, thus can't just use the\n",
    "        standard color output provided by the dataset.\n",
    "        '''\n",
    "        rgbimg = torch.zeros((3, mask.size()[0], mask.size()[1]), dtype=torch.uint8)\n",
    "        for k in self.mappingrgb:\n",
    "            rgbimg[0][mask == k] = self.mappingrgb[k][0]\n",
    "            rgbimg[1][mask == k] = self.mappingrgb[k][1]\n",
    "            rgbimg[2][mask == k] = self.mappingrgb[k][2]\n",
    "        return rgbimg\n",
    "\n",
    "    def class_to_rgb(self, mask):\n",
    "        '''\n",
    "        This function maps the classification index ids into the rgb.\n",
    "        For example after the argmax from the network, you want to find what class\n",
    "        a given pixel belongs too. This does that but just changes the color\n",
    "        so that we can compare it directly to the rgb groundtruth label.\n",
    "        '''\n",
    "        mask2class = dict((v, k) for k, v in self.mapping.items())\n",
    "        size1=206\n",
    "        size2=512\n",
    "        rgbimg = torch.zeros((3,250,506), dtype=torch.uint8) # mask.size()[0], mask.size()[1]), dtype=torch.uint8)\n",
    "        for k in mask2class:\n",
    "            rgbimg[0][mask == k] = self.mappingrgb[mask2class[k]][0]\n",
    "            rgbimg[1][mask == k] = self.mappingrgb[mask2class[k]][1]\n",
    "            rgbimg[2][mask == k] = self.mappingrgb[mask2class[k]][2]\n",
    "        return rgbimg\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # first load the RGB image\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "\n",
    "        # next load the target\n",
    "        target = Image.open(self.targets[index]).convert('L')\n",
    "\n",
    "        # If augmenting, apply random transforms\n",
    "        # Else we should just resize the image down to the correct size\n",
    "        if self.augment:\n",
    "            # Resize\n",
    "            image = TF.resize(image, size=(1024, 2048), interpolation=Image.BILINEAR)\n",
    "            target = TF.resize(target, size=(1024, 2048), interpolation=Image.NEAREST) #250 x 506] at\n",
    "            # Random crop\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(512, 1024)) \n",
    "            image = TF.crop(image, i, j, h, w)\n",
    "            target = TF.crop(target, i, j, h, w)\n",
    "            target = TF.resize(target, size=(250, 506), interpolation=Image.NEAREST)# bring target to final outuput size of the network \n",
    "            # Random horizontal flipping\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                target = TF.hflip(target)\n",
    "            # Random vertical flipping\n",
    "            # (I found this caused issues with the sky=road during prediction)\n",
    "            # if random.random() > 0.5:\n",
    "            #    image = TF.vflip(image)\n",
    "            #    target = TF.vflip(target)\n",
    "        else:\n",
    "            # Resize\n",
    "            image = TF.resize(image, size=(512,1024), interpolation=Image.BILINEAR)#512, 1024))\n",
    "            target = TF.resize(target, size=(250, 506), interpolation=Image.NEAREST)\n",
    "\n",
    "        # convert to pytorch tensors\n",
    "        # target = TF.to_tensor(target)\n",
    "        target = torch.from_numpy(np.array(target, dtype=np.uint8))\n",
    "        image = TF.to_tensor(image)\n",
    "        \n",
    "        # convert the labels into a mask\n",
    "        targetrgb = self.mask_to_rgb(target)\n",
    "        targetmask = self.mask_to_class(target)\n",
    "        targetmask = targetmask.long()\n",
    "        targetrgb = targetrgb.long()\n",
    "       \n",
    "        \n",
    "       \n",
    "        # finally return the image pair\n",
    "        return image, targetmask, targetrgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2975\n",
      "500\n",
      "1525\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataSet=CityscapesDataset('../cityscapes/','train',augment=True)\n",
    "testSet=CityscapesDataset('../cityscapes/','test')\n",
    "ValSet=CityscapesDataset('../cityscapes/','val')\n",
    "\n",
    "trainDataLoader=torch.utils.data.DataLoader(dataSet,batch_size=5, num_workers=2,shuffle=True)\n",
    "test_dataLoader=torch.utils.data.DataLoader(testSet,batch_size=2, num_workers=2,shuffle=True) \n",
    "Val_dataLoader=torch.utils.data.DataLoader(ValSet,batch_size=2, num_workers=2,shuffle=True) \n",
    "print(len(trainDataLoader.dataset))\n",
    "print(len(Val_dataLoader.dataset))\n",
    "print(len(test_dataLoader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/cfcc392c-e211-4346-b376-708130249bf4/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5ebb73b2e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAADICAYAAAAECTEjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYRElEQVR4nO3deZRU1bXH8e9u5sEBBBQZBA2JgklQ2ynGiNHgLCZO4ISKC5No1CTGgObpUxNDTDTGGPPkKYIDIFGJOEVxjs+JwREQRSXQaQIioszY3fv9cW6nqzs9VHdV9a2q+/usVavvPXWraveBtWv3uefeY+6OiIgkQ0ncAYiISOtR0hcRSRAlfRGRBFHSFxFJECV9EZEEUdIXEUmQnCV9MzvSzBab2RIzG5erzxERkfRZLubpm1kb4D3gO0AZMAcY5e4Ls/5hIiKStlxV+vsBS9z9Q3ffCkwHRuTos0REJE1tc/S+fYDlKftlwP4NHdzDzAfkKBARkWI1D1a7e8/mvCZXSd/qaas1jmRmY4GxAP2BuTkKRESkWBn8o7mvydXwThnQL2W/L1CeeoC7T3T3UncvbdbXlIiItFiukv4cYJCZDTSz9sBIYFaOPktERNKUk+Edd68wswuBJ4A2wCR3X5CLzxIRkfTlakwfd38MeCxX7y8iIs2nK3JFRBJESV9EJEGU9EVEEkRJX0QkQZT0RUQSRElfRCRBlPRFRBJESV9EJEGU9EVEEkRJX0QkQZT0RUQSRElfRCRBlPRFRBJESV9EJEGU9EVEEkRJX0QkQZT0RUQSRElfRCRBlPRFRBJESV9EJEGU9EVEEkRJX0QkQZT0RUQSRElfRCRB2mbyYjNbCqwDKoEKdy81s+7AfcAAYClwirt/mlmYIiKSDdmo9A9196HuXhrtjwOedvdBwNPRvoiI5IFcDO+MAKZE21OAE3LwGSIi0gKZJn0HnjSzeWY2Nmrb0d1XAEQ/e2X4GSIikiUZjekDB7l7uZn1Amab2bvpvjD6khgL0D/DIEREJD0ZVfruXh79XAXMBPYDVppZb4Do56oGXjvR3UvdvbRnJkGIiEjaWlzpm1kXoMTd10Xbw4FrgFnAaGBC9POhbASaFcOAgxp5/glgbuuEIiISh0yGd3YEZppZ9ftMdfe/mdkcYIaZjQGWASdnHmaGegB3AfvQ+BmGHwCfpPF+fwReTOO45cCGNI4TEWkl5u5xx0CpmeeswO4JzAAOASxL75lul90DvN/EMS8DT2UWjogkk8G8lOnyacn0RG7+ak+4QuAMYFCW3zvdL48z0zhmNfCvaPsnwOwWRSTS6jZ0hkOfhesuh8OfjjsaSVdxJv2DgVOAC8hedZ8rPaIHwDTgVMIlbSJ5rqoESqpg5/K4I5HmKK6k3wOYDOxLYV4dsAMh8R8MLI45FpEmlFTB8Cdh8KK4I5HmKJ4brnUjJMxjKMyEX60n6Q0LicSsy0a45qq4o5DmKvyk3w64HJgDHBZzLNlyKXAt0CHuQESk2BR20v8GcAPwS2A38n/8Pl0dgF+Q/RPQIpJ4+TGm3xP4e5rHriGcoO0GTCVcLVCsbiVc5bAy7kBEpFjkR9LvD3ylGcfPz1UgeeZg4HzCdc4iIllQ2MM7SXARxXOuQkRil19J/3VgfPSonrK4nnA3n6TagXDeQkQkC/Ir6X8JGBM9+kZtnYADY4soP+wO/DDuIESkGOTHmH61baJHqjaEE71J1oFw4lqkiNW9pVWxTMbLN/lV6YtIIq3YCQ5/Cnb7IDxuuiTuiIpXflX60rAjgNsIN2gTKTJTRsMzKRMW1m4fWyhFT5V+ofgm0D3uIESk0KnSF5GscMBTBuItGqSvbjNPf5zeDaoMSuJf7qPoqNIvJLotg+SBD3aFynoyx/y94cvvwcCPwuOFb8G0UTX7k86Fjwb85+s2d4Bl/Wu3/eFiOPjv8NKBsKljTn6NxFKlXyiMcGXuo3EHIsWqqp4yfNEecPt5tdtmfwcOeR7Oux2GvhH+azpw9VXwwZdqjtvcEdZ3hWW7hP3z7oChr8P9J4UvAfNQ0f96PPz5B7U/4/Pt4KWD4KD/CzHsrluNZ42SfoFw4D2ad7cKKTwL94A1GZy7GfoGdE1zXWYH5pbCluhurneeA08Or33M5o6wup4p0wv2hIdGhBOwe74Dd50VvgxqHTME/rVT7bY39oIDX4bvzoSjHoeLbo6OaWDcp/8y6LQpvd9H0pMfa+SWms/N2SK5xcGBka/DtH00zpkJJ6z41JCnDofHjm61cP7t3EmwtT2cMgOWDmz5+4yaCj0/Tu9Yt5Do19e9NqaZDp8Nzw2DinaZvU99rr4Srrw2++9bLLRGbpFbMAQuux6Ob+S2FOaw/6vQtgJe2y8kkkx89W3otha2toNX9699oq6ufsthx5WwqhcM+EfzPuezbeHNr2cUalqeOCJUpw1Zt00YWmhtD34PRk3LLOEDTDstO/E0x1PfafqY5thrPnRdH7b7L8vue4sq/fzgQFW0PZt/j9v/5uewond00szgHmBtE2/VpgLK+kL3NdC3DD7OcBWxJ4bD8Nnh4pl+y6GykTJh3K/h7Mlwxj1wwCvN+5zyneHBEzMKVQqAVdXM6qn2kxtrJ/dR06DHJ60bV6FSpZ+Jzwg3fKurO/C1HH3m69HnfgxcQkj+64F14elz7oOKtnDp7+CffWDLvkDnHMWSBcv6w5x9wzjx3H3jjkbyQe/yMKOn2hFPwOgptY/psRraf9G6cSWZkn61t4BD62nvBxyXsv81wj3uM/EJcBXwAPCvhg/rFY3NTj09fB9MPQ3uPjOMOzekbUX4ef9JsKlTqPwb4gZVbZoXemOmnh4ekiwllbWr9/Nvg8ELw/aQBTDs+Xjikvo1mfTNbBJwLLDK3feM2roD9wEDgKXAKe7+afTceMJ9MiuBi9z9iZxE3lqWE1awqvYNYASwU/2HN2oL8BJwJfBi815qwOlTw3j++q6NHOfhRN4xj8J7X278PV/dH26+qPFjuq8JP9tvhWHP1ZwE3dwRXj4Q3RUrgUoqw1TK6gLjkptg3zk1z3dfAx22xhKapKHJMX0z+xZh0OGulKR/PbDG3SeY2Tigm7v/3MwGA9OA/YCdgaeAL7t7ZWOfEfuYfhXha2pymsffAzS3oq0CriMk/PhPo2RsXVe49/SaE7ure8A1V4bhKH0RFD6rgpLoPFO7L+CXv4DOG8N+24owRKMhmfjlZEzf3V8wswF1mkcAw6LtKcBzwM+j9unuvgX4yMyWEL4AXm5OUK3OCV9P6VoEVJD+4Nh84C+ExVCKIOEDbLMevn9bzX5FGxg7ES6/Lkzf+3C32EKTFtj+U9hnXs3+V9+GcRPCtnkYd9dU4eLQ0jH9Hd19BYC7rzCz6jkifYDUeRtlUVt+qgSWAL8DPm3G6yYQfrPvA/tEbfVVuA48T/iroDyjSAtC9zXwP98PV1hepXV981ZJZajiT3wAvv1MaOu9Ao57JN64pHVk+0RufX/Y11sfmNlYYCxA//71HZEFH0SPhkwFHqLpeZB1VRL+vplJWOCkE/AnoO6c+HXAeS14/wKwqSO8+M2a4Z0rr4EPdw3bG7rEF5c0wsNY/LmT4NhHwlz4zrraNXFamvRXmlnvqMrvDayK2ssI812q9aWBGtfdJwITIYzptzCOhjkhMefyar7PU7aPa/CoglNZApV1ZvV8th1c8auaE7nru8J9p6Lx+wJQUgltKmHEQ3DHGNh2XdwRSZxamvRnAaMJAx2jCfVydftUM7uRcCJ3EPBapkG2yP1oQfFmeH0orNwxbL/wLZg4tvbzVSXwaTeU5AtJVNmfPRlO+Gs4EavKXtKZsjmNcNK2h5mVEWaYTwBmmNkYYBlwMoC7LzCzGcBCwqnOC5qauZO2L6i5arVNI5FvBR4jzKXfmJVPLgpO7Vsy3DEmXERV7enDau6GKIWtTUW4GvvKa+B7D8L2n8UdkeSTwrgNwzLgDGBBtD+cMIZe926EHxC+ft4FVNHwcY9whSzAF+3gR3+sGW9f3xW2dogvNsmNg16EM++GU+9Tsk+C4rwNgwMPA39PaZtOODl6OnAa4S+Aa4FHqP9WCkWsuoKvPqE6bRQ8f0jYLt8ZZg9v8KVSRPqUhbn0x8+C7s2ZiSaJkx9J/zNCwq7P58D4etr/BjxLWCj8PcIp4ewMJOW9tduFmTMQkv2Pf19zD/aNnWGLVhpKjE4b4dBn4VdXwNff1CkXaVp+DO+YeZJvsllXldUsbAHhrpnXXFmz/8kO8HgM93yXPOJh2uW5k8JJWiX7ZCrO4Z0E2NA5nEittqJ3zdWQEKZPrtu29eOS/HX8rHADvi6arCDNpKTfCra0rz3v/dNuIalXz3nf0CUsPadyTdJxxN/CurVK+NISSvo5sLUdPH5UTVK//jJYOLjm+aqS6E6ZSvLSTB03wUn3Q8/VcUcihUpJvwXqVu63/hDm7VOzv7U9/PWE7N6rXgRg53I45864o5BCpqSfhnl7w9IBNfu3nQ+vHFCzv6lTbhaFFkm16wcw45SaWx6LtETik35lSVgQpNq7u4fhmFRz9oWPdm3duERSWVU4ebvP/LgjkUKXuKT/6fa1lxtc8iW47vKa/Yq2sLlTq4cl0qheq8LFVyKZKqqkv7Vd7fntAP91bVhUvNqn3eDpRtaYFclH598GnXRrEcmCgr8466nDwsVKAI8dHRYET7WpE3hJRuGJxG6XpeGv0rYJuepc0lOUF2dVWZjHXn1vmQnjYPFXap5/bhis2SGW0ERaTd2/YEVaKi+T/ocD4dX9w/bn28LPfhvG2s3DSVdNhRQRaZm8SPqVJfDL8TVz3Zf1h/n7NP4aERFpvrxI+m9+Hd66WhW8SEM2dQqL3hzwatyRSKHLi1OcXqKEL9KYz7aHu8+MOwopBnmR9EVEpHUo6YuIJIiSvkiBeHd3WNkr7iik0CnpixSIZw6rfY2KSEso6YsUkOcPgfivoZdCpqQvUkBuO79mcR6RltB/H5ECsqFLzdXqIi3RZNI3s0lmtsrM3klp+28z+6eZvRE9jk55bryZLTGzxWZ2RK4CF0mitd1g8tnwRV5cVimFKJ1KfzJwZD3tv3f3odHjMQAzGwyMBIZEr7nVzHTZlUgW3TEmLOwj0hJNJn13fwFYk+b7jQCmu/sWd/8IWALsl0F8IlJHVRv4bDud0JWWyWRM/0Izeysa/ukWtfUBlqccUxa1iUgWnXUXvPSNuKOQQtTSpP9nYDdgKLACuCFqt3qOrbcgMbOxZjbXzObycQujEEmo1T1h1DRYtHvckUihaVHSd/eV7l7p7lXA/1IzhFMG9Es5tC9Q3sB7THT3UncvpWdLohBJtuX9w0pxGuaR5mhR0jez3im73wWqZ/bMAkaaWQczGwgMAl7LLEQRach1l4e5+yLpanLil5lNA4YBPcysDLgKGGZmQwlFxlLgfAB3X2BmM4CFQAVwgbtrVU+RHNncKVT7350JO66KOxopBHmxMLqVmtPSldFFhGMfhlnH139STYpXSxZG1xW5IkXglQM0m0fSo6QvUgRW94SR0+GV/XViVxqnpC9SJMr6wTl3xh2F5DslfZEisnQA3PBT2Nou7kgkXynpixSRzZ3gZ78Nq2yJ1EdJX6TYGIybAGu3izsQyUdK+iJF6PGj4NT7YPUOcUci+UZJX6QYGTw5HM6erIpfalPSFylWBo8eA7dcGHcgkk+U9EWKmcGffwCvadEViSjpixS58j7h3jwvHRh3JJIPlPRFEqC8D5z8F5jTrLu0SDFS0hdJiPI+8MNb445C4qakL5IgC4bAby/VFbtJpqQvkiCbOsNl18MffxR3JBKXJhdREZEiY/Bf14IbXPwHaFcRd0DSmlTpiyRQdcW/onfTx0pxUdIXSSg3uPAW+KR73JFIa1LSF0kqg4ePh3vO0MIrSaKkL5Jwvx4f5vCr4k8GJX2RhFu5EzxwEpx1F6zrGnc0kmtK+iICwGNHw5l3q+IvdpqyKSKBwUMnQEVbmD4Sum6IOyDJBVX6IlLLo8eEoR5V/MVJlb6I1GYw83uh4p82CrpsjDsgyaYmK30z62dmz5rZIjNbYGYXR+3dzWy2mb0f/eyW8prxZrbEzBab2RG5/AVEJDcePk4VfzFKZ3inAvipu+8BHABcYGaDgXHA0+4+CHg62id6biQwBDgSuNXM2uQieBHJIYMHT4Qxd8DGTnEHI9nSZNJ39xXuPj/aXgcsAvoAI4Ap0WFTgBOi7RHAdHff4u4fAUuA/bIct4i0klnHh1k98/eCCpVvBa9ZJ3LNbACwF/AqsKO7r4DwxQD0ig7rAyxPeVlZ1Fb3vcaa2Vwzm8vHLYhcRFqFl4SKf/9XYdK5unq30KWd9M2sK/AAcIm7f97YofW0/cf/E3ef6O6l7l5Kz3SjEJG4VLSDH/8ebr4IKjXvr2Cl9U9nZu0ICf9ed38wal5pZr2j53sDq6L2MqBfysv7AuXZCVdE4rSxC1z6O7jzHFX8hSqd2TsG3AEscvcbU56aBYyOtkcDD6W0jzSzDmY2EBgEvJa9kEUkThXtwn34b7lQFX8hSmee/kHAmcDbZvZG1HY5MAGYYWZjgGXAyQDuvsDMZgALCTN/LnD3ymwHLiLx2dgFfnoDdN4I59wJ5vWP60r+Mff4/0izUnPmxh2FiDRX5w3Qbzk8cCIMWRh3NMljMM/dS5vzGv1xJiIttrELLN4dTrof3t5T4/yFQElfRDL27h5w7COwcHDckUhTlPRFJCuW7RIq/neGqOLPZ0r6IpI1qvjzn5K+iGTVPwaE5RcXDFbFn4+U9EUk6xYNDhX/oj3ijkTqUtIXkZxYOjBU/Av3UMWfT5T0RSRnFg6BYx6Fd3ePOxKppqQvIjlVXfEv2l0Vfz5Q0heRnFuwpyr+fKGkLyKt4qNd4U8XQJVu0hMrJX0RaTWTztVCLHFT0heRVrOpM1xyE0w9TRV/XJT0RaRVbegK592uhVjioqQvIq1uc6dQ8U8bpYq/tSnpi0gs1m8DY+6AyWer4m9NSvoiEpvNncLSi9NHquJvLUr6IhKr9dvAuZNgymhV/K1BSV9EYldd8d93qir+XFPSF5G8sG5bOHsy3H2mKv5cUtIXkbyxpSNcdLMq/lxS0heRvPL5djUVv2Sfkr6I5J0tHaMx/lNU8Webkr6I5KXPtoez7oJ7T487kuKipC8ieWtrh2iMXxV/1ijpi0heW9stVPzTRsUdSXFQ0heRvLe1A1x4C/x1RNyRFD5zj39GrJl9DGwAVscdSx7ogfoB1A/V1A+B+iGo2w+7uHvP5rxBXiR9ADOb6+6lcccRN/VDoH4I1A+B+iHIRj9oeEdEJEGU9EVEEiSfkv7EuAPIE+qHQP0QqB8C9UOQcT/kzZi+iIjkXj5V+iIikmOxJ30zO9LMFpvZEjMbF3c8uWRmk8xslZm9k9LW3cxmm9n70c9uKc+Nj/plsZkdEU/U2Wdm/czsWTNbZGYLzOziqD1RfWFmHc3sNTN7M+qHq6P2RPVDNTNrY2avm9kj0X5S+2Gpmb1tZm+Y2dyoLXt94e6xPYA2wAfArkB74E1gcJwx5fj3/RawN/BOStv1wLhoexzwm2h7cNQfHYCBUT+1ift3yFI/9Ab2jra3Ad6Lft9E9QVgQNdoux3wKnBA0vohpT9+AkwFHon2k9oPS4Eeddqy1hdxV/r7AUvc/UN33wpMB4r2mjt3fwFYU6d5BDAl2p4CnJDSPt3dt7j7R8ASQn8VPHdf4e7zo+11wCKgDwnrCw/WR7vtooeTsH4AMLO+wDHA7SnNieuHRmStL+JO+n2A5Sn7ZVFbkuzo7isgJEOgV9SeiL4xswHAXoQqN3F9EQ1pvAGsAma7eyL7AbgJuAyoSmlLYj9A+OJ/0szmmdnYqC1rfdE2y8E2V333zdN0oqDo+8bMugIPAJe4++dmDd5GsWj7wt0rgaFmtj0w08z2bOTwouwHMzsWWOXu88xsWDovqaet4PshxUHuXm5mvYDZZvZuI8c2uy/irvTLgH4p+32B8phiictKM+sNEP1cFbUXdd+YWTtCwr/X3R+MmhPZFwDuvhZ4DjiS5PXDQcDxZraUMMT7bTO7h+T1AwDuXh79XAXMJAzXZK0v4k76c4BBZjbQzNoDI4FZMcfU2mYBo6Pt0cBDKe0jzayDmQ0EBgGvxRBf1lko6e8AFrn7jSlPJaovzKxnVOFjZp2Aw4F3SVg/uPt4d+/r7gMIOeAZdz+DhPUDgJl1MbNtqreB4cA7ZLMv8uBM9dGE2RsfAFfEHU+Of9dpwArgC8I39BhgB+Bp4P3oZ/eU46+I+mUxcFTc8WexH75J+BP0LeCN6HF00voC+BrwetQP7wBXRu2J6oc6fTKMmtk7iesHwkzGN6PHguqcmM2+0BW5IiIJEvfwjoiItCIlfRGRBFHSFxFJECV9EZEEUdIXEUkQJX0RkQRR0hcRSRAlfRGRBPl/tDZOagqUqMQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask=dataSet[2][1]\n",
    "plt.imshow(dataSet.class_to_rgb(mask).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#https://github.com/jarvislabsai/blog/blob/master/build_resnet34_pytorch/Building%20Resnet%20in%20PyTorch.ipynb\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
    "                     padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        #in case of size problems implement 0-padding--> https://stackoverflow.com/questions/58307036/is-there-really-no-padding-same-option-for-pytorchs-conv2d\n",
    "        #pad = nn.ZeroPad2d(conv_padding)\n",
    "        #out=pad(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def _make_layer(block, inplanes,planes, blocks, stride=1):\n",
    "    downsample = None  \n",
    "    if stride != 1 or inplanes != planes:\n",
    "        downsample = nn.Sequential(            \n",
    "            nn.Conv2d(inplanes, planes, 1, stride, bias=False),\n",
    "            nn.BatchNorm2d(planes),\n",
    "        )\n",
    "    layers = []\n",
    "    layers.append(block(inplanes, planes, stride, downsample))\n",
    "    inplanes = planes\n",
    "    for _ in range(1, blocks):\n",
    "        layers.append(block(inplanes, planes))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class ResNetEncode_UNetDecode(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.inplanes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, \n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=1, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        \n",
    "        self.x1=0\n",
    "        self.x2=0\n",
    "        self.x3=0\n",
    "        self.x4=0\n",
    "        \n",
    "      \n",
    "  \n",
    "        self.maxpoolUnet=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.convBottom1=nn.Conv2d(512, 1024, 3, 1,1)\n",
    "        self.batchBottom1= nn.BatchNorm2d(1024)\n",
    "        self.convBottom2=nn.Conv2d(1024, 1024, 3, 1,1)\n",
    "        self.batchBottom2= nn.BatchNorm2d(1024)\n",
    "                \n",
    "        #self.up=nn.Conv2d(1024,512,2,1,2)#nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.upConv= nn.Conv2d(1024,512,2,1,(8,16))\n",
    "            \n",
    "        #concatenation      \n",
    "        self.conv1U= nn.Conv2d(1024,512,3,1,1)\n",
    "        self.batch1= nn.BatchNorm2d(512)  \n",
    "        self.conv11= nn.Conv2d(512,512,3,1,1)\n",
    "        self.batch11= nn.BatchNorm2d(512)  \n",
    "        \n",
    "        self.upConv2= nn.Conv2d(512,256,2,1,(16,32))        \n",
    "         #layer 1\n",
    "        self.conv2 = nn.Conv2d(512, 256, 3, 1,1)\n",
    "        self.batch2= nn.BatchNorm2d(256)\n",
    "         #relu\n",
    "            \n",
    "        self.conv3 = nn.Conv2d(256, 256, 3, 1,1)\n",
    "        self.batch3= nn.BatchNorm2d(256)\n",
    "        self.upConv3= nn.Conv2d(256,128,2,1,(32,64))     \n",
    "        \n",
    "         #layer 2   \n",
    "        self.conv4= nn.Conv2d(256,128,3,1,1)\n",
    "        self.batch4=nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5= nn.Conv2d(128,128,3,1,1)\n",
    "        self.batch5=nn.BatchNorm2d(128)\n",
    "        self.upConv4= nn.Conv2d(128,64,2,1,(64,128))     \n",
    "        #layer 3\n",
    "        self.conv6= nn.Conv2d(128,64,3,1,1)\n",
    "        self.batch6=nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv7= nn.Conv2d(64,64,3,1,1)\n",
    "        self.batch7=nn.BatchNorm2d(64)\n",
    "        #layer 4\n",
    "      \n",
    " \n",
    "        #final convolution\n",
    "        self.conv8= nn.Conv2d(64,4,3,1,1)#20,3,1)\n",
    "        #self.batch8=nn.BatchNorm2d(1)#20\n",
    "            \n",
    "    def concatenate(self,x1,x2): #https://github.com/milesial/Pytorch-UNet/blob/6aa14cbbc445672d97190fec06d5568a0a004740/unet/unet_parts.py#L42\n",
    "        diffX = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        addBotXd=0\n",
    "        addBotYd=0\n",
    "        if(diffX%2!=0):\n",
    "          addBotXd=1\n",
    "        if(diffY%2!=0):\n",
    "          addBotYd=1\n",
    "            \n",
    "       \n",
    "        addTopX=diffX//2\n",
    "        addBotX=diffX//2+addBotXd\n",
    "        addTopY=diffY//2\n",
    "        addBotY=diffY//2+addBotYd\n",
    "               \n",
    "                 #10          #5\n",
    "        x1 = F.pad(x1,pad= (addTopY,addBotY,addTopX,addBotX)) #19, \n",
    "        \n",
    "        \n",
    "    \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None  \n",
    "   \n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        \n",
    "        self.inplanes = planes\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #downsampling (ResNet Encoder)\n",
    "        x = self.conv1(x)           \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "                 \n",
    "        x = self.layer1(x)        \n",
    "                         \n",
    "        self.x1=x\n",
    "        x = self.layer2(x)          \n",
    "        self.x2=x\n",
    "     \n",
    "        x = self.layer3(x)         \n",
    "        self.x3=x\n",
    "        \n",
    "        x = self.layer4(x)          \n",
    "        self.x4=x\n",
    "        x=self.maxpoolUnet(x)\n",
    "        #upsampling (UNet Decoder)\n",
    "        \n",
    "        x=self.convBottom1(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batchBottom1(x)\n",
    "        x=self.convBottom2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batchBottom2(x)\n",
    "       \n",
    "        #x=self.up(x)\n",
    "        x=self.upConv(x)\n",
    "        \n",
    "        x=self.concatenate(x,self.x4)\n",
    "        \n",
    "       \n",
    "        x=self.conv1U(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batch1(x)\n",
    "        x=self.conv11(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batch11(x)\n",
    "                \n",
    "        #x=self.up(x)\n",
    "        x=self.upConv2(x)\n",
    "        \n",
    "        x=self.concatenate(x,self.x3)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batch2(x)\n",
    "        x=self.conv3(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batch3(x)\n",
    "        \n",
    "        \n",
    "        #x=self.up(x)\n",
    "        x=self.upConv3(x)\n",
    "        x=self.concatenate(x,self.x2)\n",
    "        x=self.conv4(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batch4(x)\n",
    "        x=self.conv5(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batch5(x)\n",
    "        \n",
    "        #x=self.up(x)\n",
    "        x=self.upConv4(x)\n",
    "        x=self.concatenate(x,self.x1)\n",
    "        x=self.conv6(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batch6(x)\n",
    "        x=self.conv7(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.batch7(x)\n",
    "        x=self.conv8(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#set up Net Architecture\n",
    "def resnetUNet():\n",
    "    layers=[3, 4, 6, 3] #ResNet 34 \n",
    "    model = ResNetEncode_UNetDecode(BasicBlock, layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /projects/cfcc392c-e211-4346-b376-708130249bf4/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-bffd95f2b0e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 1. filter out unnecessary keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpretrained_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# 2. overwrite entries in the existing state dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    948\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    " # Look up graphic cards\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "#Initalize model\n",
    "model=resnetUNet().to(device)\n",
    "#load pre trained ResNet model\n",
    "#model.load_state_dict(torch.load('model_ResNet34_40Epoch (1).pth'), strict=False)\n",
    "pretrained_dict = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=True)\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "#UNet=UNetDecoder().to(device)\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(3,512, 1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#print(model) https://towardsdatascience.com/everything-you-need-to-know-about-saving-weights-in-pytorch-572651f3f8de\n",
    "#for name, param in model.named_parameters():\n",
    "#    print('name: ', name)\n",
    "#    print(type(param))\n",
    "#    print(param)\n",
    "#    print(\"======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e151bc148e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize learning scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Initialize optimizer\n",
    "\n",
    "# Initialize learning scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from livelossplot import PlotLosses #https://github.com/stared/livelossplot/blob/master/examples/pytorch.ipynb\n",
    "\n",
    "\n",
    "\n",
    "# Define training loop\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    plotlosses = PlotLosses()\n",
    "    model.train()\n",
    "    logs = {}\n",
    "    epochLoss=[]\n",
    "    epochAcc=[]\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_count=0\n",
    "    for batch_idx, (data, target,labelrgb) in enumerate(train_loader):\n",
    "        \n",
    "       \n",
    "        data, target = data.to(device), target.to(device,dtype=torch.int64)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "       \n",
    "        # TK_HERE, we need to check what output is being generated, and what out target is,\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        running_loss += loss.detach() * data.size(0)\n",
    "        epochLoss.append(loss)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        running_corrects += torch.sum(preds == target.data)\n",
    "        running_count +=len(target.data)\n",
    "        \n",
    "      \n",
    "        \n",
    "        acc= running_corrects/running_count\n",
    "        epochAcc.append(acc)\n",
    "        \n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "       \n",
    "    return sum(epochLoss)/len(epochLoss),sum(epochAcc)/len(epochAcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-af90b19fb5ad>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-af90b19fb5ad>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    running_corrects = 0https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def Validation(model, device, Val_loader):\n",
    "    plotlosses = PlotLosses()\n",
    "    logs = {}\n",
    "    epochLoss=[]\n",
    "    epochAcc=[]\n",
    "    epochClassLoss = {}\n",
    "    IOU=[]\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    running_count=0\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "   \n",
    "        net=model\n",
    "      \n",
    "\n",
    "        for batch_idx, (data, target,labelrgb) in tqdm(enumerate(Val_loader)):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)    \n",
    "            output = net(data)\n",
    "            #collect all IoU scores throughout the dataset & build average after going through all images\n",
    "            IoUscore=IoU(output, target,Val_loader.dataset)\n",
    "           \n",
    "            IOU.append(IoUscore)\n",
    "            loss = F.cross_entropy(output, target)# Initialize learning scheduler\n",
    "            running_loss += loss.detach() * data.size(0)\n",
    "            \n",
    "            \n",
    "            epochLoss.append(loss)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            running_corrects += torch.sum(preds == target.data)\n",
    "            running_count +=len(target.data)\n",
    "\n",
    "            \n",
    "            targetClassid = target.data # Here\n",
    "            predictedClassID = preds # here\n",
    "            \n",
    "           \n",
    "            acc= running_corrects/running_count\n",
    "            epochAcc.append(acc)\n",
    "            #return averaged loss/accuracy over the validation set\n",
    "\n",
    "                \n",
    "            \n",
    "    return sum(epochLoss)/len(epochLoss),sum(epochAcc)/len(epochAcc), epochClassLoss,sum(IOU)/len(IOU)\n",
    "\n",
    "#resources\n",
    "#https://www.jeremyjordan.me/evaluating-image-segmentation-models/\n",
    "#see https://github.com/milesial/Pytorch-UNet/blob/6aa14cbbc445672d97190fec06d5568a0a004740/train.py\n",
    "#or https://medium.com/@cyborg.team.nitr/miou-calculation-4875f918f4cb\n",
    "def IoU(prediction,target, dataset):\n",
    "    target=target.cpu()\n",
    "    prediction=prediction.cpu()\n",
    "   \n",
    "    targetN=torch.zeros((2,3,250,506))\n",
    "    maskN=torch.zeros(2,3,250,506)\n",
    "    targetN[0]= dataset.class_to_rgb(target[0])#colorize mask to be able to compare \n",
    "    targetN[1]= dataset.class_to_rgb(target[1])\n",
    "    \n",
    "    mask=processPred(prediction)\n",
    "    \n",
    "    maskN[0]= dataset.class_to_rgb(torch.squeeze(mask[0],0))\n",
    "    maskN[1]= dataset.class_to_rgb(torch.squeeze(mask[1],0))\n",
    "    \n",
    "    \n",
    "    intersection = np.logical_and(targetN,maskN).numpy() #map differences of prediction and target\n",
    "    union = np.logical_or(targetN, maskN).numpy()        #map similiarities of prediction and target\n",
    "    iou_score = (np.sum(intersection) / np.sum(union))*100\n",
    "    \n",
    "    return iou_score\n",
    "\n",
    "def processPred(mask):\n",
    "   \n",
    "     #mapping of the 4 used classes to 1 mask to calculate IoU score\n",
    "     mask0=np.where(mask[0][0]==True,0,0)   \n",
    "     mask1=np.where(mask[0][1]==True,2,0)\n",
    "     mask2=np.where(mask[0][2]==True,3,0)\n",
    "     mask3=np.where(mask[0][3]==True,4,0)   \n",
    "     a=mask0|mask1|mask2|mask3\n",
    "    \n",
    "     mask01=np.where(mask[1][0]==True,0,0)   \n",
    "     mask11=np.where(mask[1][1]==True,2,0)\n",
    "     mask21=np.where(mask[1][2]==True,3,0)\n",
    "     mask31=np.where(mask[1][3]==True,4,0)   \n",
    "     b=mask01|mask11|mask21|mask31\n",
    "     \n",
    "   \n",
    "     #give masks coloring and stack them on top of each other to get shape of 1 batch (consisting of 2 Label files)         \n",
    "     Mask1 =torchvision.transforms.functional.to_tensor(a)\n",
    "     Mask2 =torchvision.transforms.functional.to_tensor(b)\n",
    "    \n",
    "     return torch.stack((Mask1,Mask2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#lossV, accV, epochClassLoss, IoUscore =Validation(model,device, Val_dataLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Define test loop\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def saveModel(model,path):\n",
    "    torch.save(model.state_dict(), path) \n",
    "    \n",
    "import copy\n",
    "\n",
    "from livelossplot import PlotLosses \n",
    "plotlosses = PlotLosses()\n",
    "torch.cuda.empty_cache()\n",
    "logs = {}\n",
    "#best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#best_acc = 0.0\n",
    "BestIoU=0\n",
    "BestModel=model\n",
    "for epoch in range(1, 25): #40):\n",
    "    loss,acc=train(model, device, trainDataLoader, optimizer, epoch)\n",
    "    lossV, accV, epochClassLoss, IoUscore =Validation(model,device, Val_dataLoader)\n",
    "    if IoUscore>BestIoU:\n",
    "        BestIoU=IoUscore\n",
    "        BestModel=model\n",
    "    \n",
    "    prefix=''\n",
    "   \n",
    "    \n",
    "    logs[prefix + 'log loss'] = loss\n",
    "    #logs[prefix + 'acc']=acc\n",
    "    prefix='val_'\n",
    "    logs[prefix + 'log loss'] = lossV\n",
    "    logs[prefix + 'IoU']=IoUscore\n",
    "    plotlosses.update(logs)\n",
    "    plotlosses.send()\n",
    "    #test(model, device, test_dataLoader)\n",
    "    scheduler.step()\n",
    "\n",
    " \n",
    "path1=\"/projects/cfcc392c-e211-4346-b376-708130249bf4/bestPerformance2_model_UNet.pth\"  \n",
    "path2=\"/projects/cfcc392c-e211-4346-b376-708130249bf4/last_model2_UNet.pth\"  \n",
    "saveModel(BestModel,path1)   \n",
    "saveModel(model,path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#see https://github.com/milesial/Pytorch-UNet/blob/6aa14cbbc445672d97190fec06d5568a0a004740/predict.py\n",
    "def predict(image,model):\n",
    "     out_threshold=0.5\n",
    "     img = image.unsqueeze(0) ##add a dimension [1,3,512,1024] to simulate dataloader(batch size)\n",
    "     img = img.to(device=device, dtype=torch.float32)   \n",
    "    \n",
    "     \n",
    "     with torch.no_grad():\n",
    "         pred=model(img)\n",
    "         probs = F.softmax(pred, dim=1)\n",
    "         probs = probs.squeeze(0)\n",
    "         tf = transforms.Compose(\n",
    "                    [\n",
    "                        transforms.ToPILImage(),\n",
    "                        transforms.Resize(size=(250, 506), interpolation=Image.NEAREST),\n",
    "                        transforms.ToTensor()\n",
    "                    ]\n",
    "                )\n",
    "         \n",
    "         probs = tf(probs.cpu())\n",
    "         \n",
    "         full_mask = probs.squeeze().cpu().numpy()\n",
    "         return full_mask > out_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "mask=dataSet.class_to_rgb(dataSet[1][1])\n",
    "orgmask=mask\n",
    "\n",
    "\n",
    "\n",
    "predmask,gtmask=maskDecoder(predict(dataSet[1][0],model),dataSet)\n",
    "intersection = np.logical_and(predmask,orgmask).numpy() #map differences of prediction and target\n",
    "union = np.logical_or(predmask,orgmask).numpy()        #map similiarities of prediction and target\n",
    "iou_score = (np.sum(intersection) / np.sum(union))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(iou_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def LoadModel(modeltype):\n",
    "    Path=''\n",
    "    if modeltype=='best':\n",
    "        Path=\"/projects/cfcc392c-e211-4346-b376-708130249bf4/bestPerformance_model_UNet.pth\"\n",
    "    else:\n",
    "        Path=\"/projects/cfcc392c-e211-4346-b376-708130249bf4/last_model_UNet.pth\"\n",
    "    model =resnetUNet().to(device)    \n",
    "    model.load_state_dict(torch.load(Path))\n",
    "    model.eval()\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def maskDecoder(mask,dataset):\n",
    "\n",
    "    mask0=np.where(mask[0]==True,0,0)   \n",
    "    mask1=np.where(mask[1]==True,1,0)\n",
    "    mask2=np.where(mask[2]==True,2,0)\n",
    "    mask3=np.where(mask[3]==True,3,0)   \n",
    "    a=mask0|mask1|mask2|mask3\n",
    "    RGB_colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0]]  #0=unlabled(red) #1=road(green) 2=sky(blue)3=car(yellow)\n",
    "    a1=np.zeros((3,250,506))\n",
    "    b=np.zeros((3,250,506))\n",
    "    c=np.zeros((3,250,506))\n",
    "    d=np.zeros((3,250,506))\n",
    "    a1[0]=np.where(a==0,RGB_colors[0][0],[0]) # dont consider a[1] a[2] because the color values from the first color(RGB_colors[0]) are 0\n",
    "    b[1]=np.where(a==0,RGB_colors[1][1],[0])\n",
    "    c[2]=np.where(a==0,RGB_colors[2][2],[0])\n",
    "    d[0]=np.where(a==0,RGB_colors[3][0],[0])\n",
    "    \n",
    "   \n",
    "    imgGroundTruth=a1+b+c+d#a1|b|c|d\n",
    "             \n",
    "    \n",
    "    imagecolored = dataSet.class_to_rgb(a)\n",
    "\n",
    "    return imagecolored,imgGroundTruth\n",
    "\n",
    "test=dataSet[1][0]\n",
    "predictionMask,groundTruthMask,=maskDecoder(predict(test,model),dataSet)\n",
    "groundTruthMask=np.transpose(groundTruthMask,(1,2,0))\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "\n",
    "axs[0].imshow(predictionMask.permute(1,2,0))\n",
    "axs[1].imshow((groundTruthMask))\n",
    "axs[2].imshow(test.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#select uploaded image and let the model predict the masks\n",
    "PathImg='IMG-20191025-WA0017.jpg'\n",
    "tf = transforms.Compose([  transforms.ToPILImage(),\n",
    "                                    transforms.Resize(size=(512, 1024), interpolation=Image.BILINEAR),\n",
    "                                    transforms.ToTensor()\n",
    "                                 ])\n",
    "loadedModel=LoadModel('last')\n",
    "image =tf(plt.imread(PathImg))\n",
    "predictionMask,groundTruthMask,=maskDecoder(predict(image,loadedModel))\n",
    "groundTruthMask=np.transpose(groundTruthMask,(1,2,0))\n",
    "\n",
    "fig, axs = plt.subplots(3)\n",
    "axs[0].imshow(image.permute(1,2,0))\n",
    "axs[1].imshow(predictionMask)\n",
    "axs[2].imshow((groundTruthMask))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda's Python for Pytorch with Cuda",
   "language": "python",
   "name": "python-conda-cuda-pytorch",
   "resource_dir": "/usr/local/share/jupyter/kernels/python-conda-cuda-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
